diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
index 9953754..269f9ae 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
@@ -855,7 +855,7 @@ public String getClientName() {
     return clientName;
   }
 
-  void checkOpen() throws IOException {
+  public void checkOpen() throws IOException {
     if (!clientRunning) {
       IOException result = new IOException("Filesystem closed");
       throw result;
@@ -3726,4 +3726,8 @@ void addRetLenToReaderScope(TraceScope scope, int retLen) {
   Tracer getTracer() {
     return tracer;
   }
+
+  public FileSystem.Statistics getStats() {
+    return stats;
+  }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
index d626d35..a218734 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java
@@ -96,9 +96,9 @@
   @VisibleForTesting
   public static boolean tcpReadsDisabledForTesting = false;
   private long hedgedReadOpsLoopNumForTesting = 0;
-  private final DFSClient dfsClient;
-  private AtomicBoolean closed = new AtomicBoolean(false);
-  private final String src;
+  protected final DFSClient dfsClient;
+  protected AtomicBoolean closed = new AtomicBoolean(false);
+  protected final String src;
   private final boolean verifyChecksum;
 
   // state by stateful read only:
@@ -243,6 +243,25 @@ void clear() {
    */
   private int failures = 0;
 
+  // Work around class for thread unsafety and general dubiousness of
+  // failures data-member. Its problems:
+  // * unclear what happens when several threads do read requests:
+  //   - new readers reset failure to 0 at random times;
+  //   - reaching the limit will abort all threads.
+  // * why should the count be reset for every read and not for every block?
+  // Declaring a local counter of the same name removes thread-unsafety and
+  // makes every block search use predictable number of queries.  
+  private static class Counter {
+    private int i = 0;
+    
+    public void inc() {
+      i++;
+    }
+    public int get() {
+      return i;
+    }
+  }
+
   /* XXX Use of CocurrentHashMap is temp fix. Need to fix 
    * parallel accesses to DFSInputStream (through ptreads) properly */
   private final ConcurrentHashMap<DatanodeInfo, DatanodeInfo> deadNodes =
@@ -254,7 +273,7 @@ void addToDeadNodes(DatanodeInfo dnInfo) {
     deadNodes.put(dnInfo, dnInfo);
   }
   
-  DFSInputStream(DFSClient dfsClient, String src, boolean verifyChecksum
+  public DFSInputStream(DFSClient dfsClient, String src, boolean verifyChecksum
                  ) throws IOException, UnresolvedLinkException {
     this.dfsClient = dfsClient;
     this.verifyChecksum = verifyChecksum;
@@ -554,7 +573,7 @@ private LocatedBlock fetchBlockAt(long offset, long length, boolean useCache)
    * @return consequent segment of located blocks
    * @throws IOException
    */
-  private List<LocatedBlock> getBlockRange(long offset,
+  protected List<LocatedBlock> getBlockRange(long offset,
       long length)  throws IOException {
     // getFileLength(): returns total file length
     // locatedBlocks.getFileLength(): returns length of completed blocks
@@ -631,6 +650,7 @@ private synchronized DatanodeInfo blockSeekTo(long target) throws IOException {
     
     boolean connectFailedOnce = false;
 
+    Counter failures = new Counter();
     while (true) {
       //
       // Compute desired block
@@ -639,7 +659,7 @@ private synchronized DatanodeInfo blockSeekTo(long target) throws IOException {
       assert (target==pos) : "Wrong postion " + pos + " expect " + target;
       long offsetIntoBlock = target - targetBlock.getStartOffset();
 
-      DNAddrPair retval = chooseDataNode(targetBlock, null);
+      DNAddrPair retval = chooseDataNode(targetBlock, null, failures);
       chosenNode = retval.info;
       InetSocketAddress targetAddr = retval.addr;
       StorageType storageType = retval.storageType;
@@ -998,7 +1018,7 @@ private void addIntoCorruptedBlockMap(ExtendedBlock blk, DatanodeInfo node,
   }
 
   private DNAddrPair chooseDataNode(LocatedBlock block,
-      Collection<DatanodeInfo> ignoredNodes) throws IOException {
+      Collection<DatanodeInfo> ignoredNodes, Counter failures) throws IOException {
     while (true) {
       try {
         return getBestNodeDNAddrPair(block, ignoredNodes);
@@ -1006,7 +1026,7 @@ private DNAddrPair chooseDataNode(LocatedBlock block,
         String errMsg = getBestNodeDNAddrPairErrorString(block.getLocations(),
           deadNodes, ignoredNodes);
         String blockInfo = block.getBlock() + " file=" + src;
-        if (failures >= dfsClient.getMaxBlockAcquireFailures()) {
+        if (failures.get() >= dfsClient.getMaxBlockAcquireFailures()) {
           String description = "Could not obtain block: " + blockInfo;
           DFSClient.LOG.warn(description + errMsg
               + ". Throwing a BlockMissingException");
@@ -1032,9 +1052,9 @@ private DNAddrPair chooseDataNode(LocatedBlock block,
           // will wait 6000ms grace period before retry and the waiting window is
           // expanded to 9000ms. 
           final int timeWindow = dfsClient.getConf().timeWindow;
-          double waitTime = timeWindow * failures +       // grace period for the last round of attempt
-            timeWindow * (failures + 1) * DFSUtil.getRandom().nextDouble(); // expanding time window for each failure
-          DFSClient.LOG.warn("DFS chooseDataNode: got # " + (failures + 1) +
+          double waitTime = timeWindow * failures.get() +       // grace period for the last round of attempt
+            timeWindow * (failures.get() + 1) * DFSUtil.getRandom().nextDouble(); // expanding time window for each failure
+          DFSClient.LOG.warn("DFS chooseDataNode: got # " + (failures.get() + 1) +
               " IOException, will wait for " + waitTime + " msec.");
           Thread.sleep((long)waitTime);
         } catch (InterruptedException e) {
@@ -1044,7 +1064,7 @@ private DNAddrPair chooseDataNode(LocatedBlock block,
         deadNodes.clear(); //2nd option is to remove only nodes[blockId]
         openInfo();
         block = getBlockAt(block.getStartOffset(), false);
-        failures++;
+        failures.inc();
         continue;
       }
     }
@@ -1116,12 +1136,13 @@ private static String getBestNodeDNAddrPairErrorString(
     return errMsgr.toString();
   }
 
-  private void fetchBlockByteRange(LocatedBlock block, long start, long end,
+  protected void fetchBlockByteRange(LocatedBlock block, long start, long end,
       byte[] buf, int offset,
       Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap)
       throws IOException {
+    Counter failures = new Counter();
     while (true) {
-      DNAddrPair addressPair = chooseDataNode(block, null);
+      DNAddrPair addressPair = chooseDataNode(block, null, failures);
       block = addressPair.block;
       try {
         actualGetFromOneDataNode(addressPair, start, end, buf, offset,
@@ -1272,6 +1293,7 @@ private void hedgedFetchBlockByteRange(LocatedBlock block, long start,
     ByteBuffer bb = null;
     int len = (int) (end - start + 1);
     int hedgedReadId = 0;
+    Counter failures = new Counter();
     while (true) {
       // see HDFS-6591, this metric is used to verify/catch unnecessary loops
       hedgedReadOpsLoopNumForTesting++;
@@ -1280,7 +1302,7 @@ private void hedgedFetchBlockByteRange(LocatedBlock block, long start,
       if (futures.isEmpty()) {
         // chooseDataNode is a commitment. If no node, we go to
         // the NN to reget block locations. Only go here on first read.
-        chosenNode = chooseDataNode(block, ignored);
+        chosenNode = chooseDataNode(block, ignored, failures);
         // Latest block, if refreshed internally
         block = chosenNode.block;
         bb = ByteBuffer.allocate(len);
@@ -1325,7 +1347,7 @@ private void hedgedFetchBlockByteRange(LocatedBlock block, long start,
           try {
             chosenNode = getBestNodeDNAddrPair(block, ignored);
           } catch (IOException ioe) {
-            chosenNode = chooseDataNode(block, ignored);
+            chosenNode = chooseDataNode(block, ignored, failures);
           }
           // Latest block, if refreshed internally
           block = chosenNode.block;
@@ -1526,7 +1548,7 @@ private int pread(long position, byte[] buffer, int offset, int length)
    * @param corruptedBlockMap map of corrupted blocks
    * @param dataNodeCount number of data nodes who contains the block replicas
    */
-  private void reportCheckSumFailure(
+  protected void reportCheckSumFailure(
       Map<ExtendedBlock, Set<DatanodeInfo>> corruptedBlockMap, 
       int dataNodeCount) {
     if (corruptedBlockMap.isEmpty()) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
index eb77cd3..d3c8822 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java
@@ -113,7 +113,7 @@
   private String homeDirPrefix =
       DFSConfigKeys.DFS_USER_HOME_DIR_PREFIX_DEFAULT;
 
-  DFSClient dfs;
+  protected DFSClient dfs;
   private boolean verifyChecksum = true;
 
   private DFSOpsCountStatistics storageStatistics;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
index 22e6a3f..9217570 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
@@ -478,13 +478,15 @@ public static InetSocketAddress getAddress(URI filesystemURI) {
           "Invalid URI for NameNode address (check %s): %s has no authority.",
           FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString()));
     }
-    if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(
+    // comment this out so we can use schemes other than hdfs!!
+    /* if (!HdfsConstants.HDFS_URI_SCHEME.equalsIgnoreCase(
         filesystemURI.getScheme())) {
       throw new IllegalArgumentException(String.format(
           "Invalid URI for NameNode address (check %s): %s is not of scheme '%s'.",
           FileSystem.FS_DEFAULT_NAME_KEY, filesystemURI.toString(),
           HdfsConstants.HDFS_URI_SCHEME));
     }
+    */
     return getAddress(authority);
   }
 
